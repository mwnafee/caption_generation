{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30097,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/flickr-image-dataset/flickr30k_images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata = pd.read_csv('../input/flickr-image-dataset/flickr30k_images/results.csv',delimiter='|',engine='python')\nmetadata = metadata.dropna()\nis_NaN = metadata.isnull()\nrow_has_NaN = is_NaN.any(axis=1)\nrows_with_NaN = metadata[row_has_NaN]\nprint(rows_with_NaN)\nmetadata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of Samples","metadata":{}},{"cell_type":"code","source":"len(metadata['image_name'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read the Data","metadata":{}},{"cell_type":"code","source":"def load_image(name):\n    img = image.load_img(name,target_size=(32,32,3))\n    img = image.img_to_array(img)\n    #img = img/255\n    #plt.imshow(img)\n    img = np.reshape(img,(32*32*3))\n    return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_arr = []\nsentence_arr = []\nfor ind in range(5000):\n    if ind % 5 != 0:\n        continue\n    image_location = (metadata.iloc[ind,:]['image_name'])\n    sentence = (metadata.iloc[ind,:][' comment'])\n    \n    \n    image_arr.append(load_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location)) )\n    sentence_arr.append('<SOS>'+sentence+'<EOS>')\n    \n        \nImages =  np.array(image_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess\nConverting to Word Embeddings\ncreate padding and make equal length\nVocabulary\nThe complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with.\n\n## Tokenize (IMPLEMENTATION)\nFor a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n\nWe can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n\nTurn each sentence into a sequence of words ids using Keras's Tokenizer function. Use this function to tokenize english_sentences and french_sentences in the cell below.\n\nRunning the cell will run tokenize on sample data and show output for debugging.","metadata":{}},{"cell_type":"code","source":"def tokenize(x):\n    \"\"\"\n    Tokenize x\n    :param x: List of sentences/strings to be tokenized\n    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n    \"\"\"\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(x)\n    t=tokenizer.texts_to_sequences(x)\n    # TODO: Implement\n    return t, tokenizer\n\n# Tokenize Example output\ntext_sentences = [\n    'The quick brown fox jumps over the lazy dog .',\n    'By Jove , my quick study of lexicography won a prize .',\n    'This is a short sentence .']\ntext_tokenized, text_tokenizer = tokenize(text_sentences)\nprint(text_tokenizer.word_index)\nprint()\nfor sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(sent))\n    print('  Output: {}'.format(token_sent))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding (IMPLEMENTATION)\nWhen batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n\nMake sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function.","metadata":{}},{"cell_type":"code","source":"def pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    # TODO: Implement\n    padding=pad_sequences(x,padding='post',maxlen=length)\n    return padding\n\n# Pad Tokenized output\ntest_pad = pad(text_tokenized)\nfor sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(np.array(token_sent)))\n    print('  Output: {}'.format(pad_sent))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(sentences):\n    text_tokenized, text_tokenizer = tokenize(sentences)\n    text_pad = pad(text_tokenized)\n    return text_pad, text_tokenizer\n\nSentence , token_Sentence = preprocess(sentence_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Sentence vocabulary size:\", len(token_Sentence.word_index))\nprint(\"Sentence Longest sentence size:\", len(Sentence[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Images.shape , Sentence.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and batch data\nThis tutorial uses torchtext to generate Wikitext-2 dataset. The vocab object is built based on the train dataset and is used to numericalize tokens into tensors. Starting from sequential data, the batchify() function arranges the dataset into columns, trimming off any tokens remaining after the data has been divided into batches of size batch_size. For instance, with the alphabet as the sequence (total length of 26) and a batch size of 4, we would divide the alphabet into 4 sequences of length 6:\n\n\n \n \n \nThese columns are treated as independent by the model, which means that the dependence of G and F can not be learned, but allows more efficient batch processing.","metadata":{}},{"cell_type":"code","source":"def create_batch(src, tar , batchsize , i):\n    src, tar =  np.transpose(src[(i-1)*batchsize : (i-1)*batchsize + batchsize]) , np.transpose(tar[(i-1)*batchsize : (i-1)*batchsize + batchsize])\n    return torch.tensor(src).long(),torch.tensor(tar).long()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        embedding_size,\n        src_vocab_size,\n        trg_vocab_size,\n        src_pad_idx,\n        num_heads,\n        num_encoder_layers,\n        num_decoder_layers,\n        forward_expansion,\n        dropout,\n        max_len_s,\n        max_len_t,\n        device,\n    ):\n        super(Transformer, self).__init__()\n        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n        self.src_position_embedding = nn.Embedding(max_len_s, embedding_size)\n        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n        self.trg_position_embedding = nn.Embedding(max_len_t, embedding_size)\n\n        self.device = device\n        self.transformer = nn.Transformer(\n            embedding_size,\n            num_heads,\n            num_encoder_layers,\n            num_decoder_layers,\n            forward_expansion,\n            dropout,\n        )\n        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.src_pad_idx = src_pad_idx\n\n    def make_src_mask(self, src):\n        src_mask = src.transpose(0, 1) == self.src_pad_idx\n\n        # (N, src_len)\n        return src_mask.to(self.device)\n\n    def forward(self, src, trg):\n        src_seq_length, N = src.shape\n        trg_seq_length, N = trg.shape\n\n        src_positions = (\n            torch.arange(0, src_seq_length)\n            .unsqueeze(1)\n            .expand(src_seq_length, N)\n            .to(self.device)\n        )\n\n        trg_positions = (\n            torch.arange(0, trg_seq_length)\n            .unsqueeze(1)\n            .expand(trg_seq_length, N)\n            .to(self.device)\n        )\n\n        embed_src = self.dropout(\n            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n        )\n        embed_trg = self.dropout(\n            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n        )\n\n        src_padding_mask = self.make_src_mask(src)\n        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n        out = self.transformer(\n            embed_src,\n            embed_trg,\n            src_key_padding_mask=src_padding_mask,\n            tgt_mask=trg_mask,\n        )\n        out = self.fc_out(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model hyperparameters\nsrc_vocab_size = 256\ntrg_vocab_size = len(token_Sentence.word_index)\nembedding_size = 512\nnum_heads = 8\nnum_encoder_layers = 3\nnum_decoder_layers = 3\ndropout = 0.10\nmax_len_s = Images.shape[1]\nmax_len_t = len(Sentence[0])\nforward_expansion = 4\nsrc_pad_idx = 0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training hyperparameters\nnum_epochs = 10000\nlearning_rate = 3e-4\nbatch_size = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Transformer(\n    embedding_size,\n    src_vocab_size,\n    trg_vocab_size,\n    src_pad_idx,\n    num_heads,\n    num_encoder_layers,\n    num_decoder_layers,\n    forward_expansion,\n    dropout,\n    max_len_s,\n    max_len_t,\n    device,\n).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, factor=0.1, patience=10, verbose=True\n)\n\npad_idx = 0\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx).cuda()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndef train():\n    model.train() # Turn on the train mode\n    total_loss = 0\n    start_time = time.time()\n    for i in range(1, 999):\n        src,tar = create_batch(Images,Sentence, batch_size , i)\n        src = src.to(device)\n        tar = tar.to(device)\n        optimizer.zero_grad()\n        output = model(src,tar)\n        loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n        cur_loss = loss.item()\n        total_loss += cur_loss\n        log_interval = 100\n        if i % log_interval == 0 and i > 0:\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  's/batch {:5.2f} | '\n                  'loss {:5.2f} | ppl {:8.2f}'.format(\n                    epoch, i, (src.shape[1]) // batch_size, \n                    elapsed  / log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            start_time = time.time()\n    return total_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nfor epoch in range(1, 3):\n    epoch_start_time = time.time()\n    loss = train()\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | Training loss {:5.2f} | '\n          .format(epoch, (time.time() - epoch_start_time),\n                                     loss))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate\nThe following steps are used for evaluation:\n\n1. Encode english sentence\n2. Add tokens of start and end\n3. Decoder input is start token SOS\n4. Get the padded version of enoded sentences\n5. create mask\nTill get eos token calculate or create sentence","metadata":{}},{"cell_type":"code","source":"def display_image(name):\n    img = image.load_img(name,target_size=(512,512,3))\n    img = image.img_to_array(img)\n    img = img/255\n    plt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(index):\n    image_location, sent = metadata.iloc[index,0],metadata.iloc[index,2]\n    image_arr = []\n    img = load_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n    image_arr.append(img)\n    img_arr = np.array(image_arr)\n    sentence = []\n    sentence.append(sent)\n    sentence[0] = '<SOS> '+sentence[0]+'<EOS>'\n    sentence = pad(token_Sentence.texts_to_sequences(sentence) , length = max_len_t)\n    src , tar = create_batch(img_arr,sentence, 1,1)\n    src = src.to(device)\n    tar = tar.to(device)\n    model.eval()\n    output =  model(src,tar)\n    loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n    sentence_formed = ''\n    val, ind = torch.max(output.view(-1, output.shape[2]), 1)\n    for word in ind:\n        #print('--->'+sentence_formed+'    '+str(word.item()))\n        if word.item() == 3: # EOS\n                break\n        for key, value in token_Sentence.word_index.items():\n            #print(value == word.item())\n            if value == word.item() and value != 2: # sos\n                sentence_formed = sentence_formed + key +' '\n                break\n    display_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n    return sentence_formed , loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}